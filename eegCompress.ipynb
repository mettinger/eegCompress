{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget \n",
    "\n",
    "import numpy as np\n",
    "import mne\n",
    "#mne.datasets.sample.data_path()\n",
    "\n",
    "import torch\n",
    "import eegCompressModels\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /teamspace/uploads/ExampleLTMFiles/SVD001.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1424/3455569217.py:2: RuntimeWarning: Omitted 9 annotation(s) that were outside data range.\n",
      "  raw = mne.io.read_raw_edf('/teamspace/uploads/ExampleLTMFiles/SVD001.edf')\n"
     ]
    }
   ],
   "source": [
    "#raw = mne.io.read_raw_fif('./mne_data/MNE-sample-data/MEG/sample/sample_audvis_raw.fif')\n",
    "raw = mne.io.read_raw_edf('/teamspace/uploads/ExampleLTMFiles/SVD001.edf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RawEDF | SVD001.edf, 46 x 1276416 (4986.0 s), ~48 KiB, data not loaded>\n",
      "<Info | 8 non-empty values\n",
      " bads: []\n",
      " ch_names: Fp1, F7, T7, P7, O1, F3, C3, P3, A1, Fz, Cz, Fp2, F8, T8, P8, ...\n",
      " chs: 46 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 128.0 Hz\n",
      " meas_date: 2001-01-01 04:46:55 UTC\n",
      " nchan: 46\n",
      " projs: []\n",
      " sfreq: 256.0 Hz\n",
      " subject_info: <subject_info | his_id: SVD001>\n",
      ">\n"
     ]
    }
   ],
   "source": [
    "print(raw)\n",
    "print(raw.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 1276416)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chanList = range(0,19)\n",
    "nChannel = len(chanList)\n",
    "\n",
    "data = raw.get_data()[chanList] #eeg channels\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "950 190\n"
     ]
    }
   ],
   "source": [
    "numSampleInput = 50\n",
    "outSample = 10\n",
    "\n",
    "inSize = nChannel * numSampleInput\n",
    "outSize = nChannel * outSample\n",
    "print(inSize, outSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25528, 950)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nBlock = data.size // inSize\n",
    "resizeSamples = nBlock * numSampleInput\n",
    "\n",
    "dataReshape = np.reshape(data[:, 0:resizeSamples], (inSize, -1), order='F').transpose()\n",
    "dataReshape = dataReshape.astype('float32')\n",
    "dataReshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(torch.from_numpy(dataReshape), torch.from_numpy(np.arange(0, dataReshape.shape[0])))\n",
    "batch_size = 32\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = eegCompressModels.AE(inSize, outSize)\n",
    "loss_function = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "\t\t\t\t\t\t\tlr = 1e-1,\n",
    "\t\t\t\t\t\t\tweight_decay = 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "outputs = []\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "\tfor (image, _) in loader:\n",
    "\t\t\n",
    "\t\t# Output of Autoencoder\n",
    "\t\treconstructed = model(image.to(torch.float32))\n",
    "\t\t\n",
    "\t\t# Calculating the loss function\n",
    "\t\tloss = loss_function(reconstructed, image)\n",
    "\t\t\n",
    "\t\t# The gradients are set to zero,\n",
    "\t\t# the gradient is computed and stored.\n",
    "\t\t# .step() performs parameter update\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\t\n",
    "\t\t# Storing the losses in a list for plotting\n",
    "\t\tlosses.append(loss)\n",
    "\t\toutputs.append((epochs, image, reconstructed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.1M\n",
      "drwxr-xr-x 1 jettinger35 jettinger35 4.0K Jan  6 20:06 __pycache__\n",
      "-rwxr--r-- 1 jettinger35 jettinger35 6.0K Jan  6 22:15 eegCompress.ipynb\n",
      "-rwxr--r-- 1 jettinger35 jettinger35  845 Jan  3 00:32 eegCompressModels.py\n",
      "drwxr-xr-x 1 jettinger35 jettinger35 4.0K Jan  6 20:06 mne_data\n",
      "-rw-r--r-- 1 jettinger35 jettinger35 1.1M Jan  6 22:15 savedModel\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'savedModel')\n",
    "!ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
